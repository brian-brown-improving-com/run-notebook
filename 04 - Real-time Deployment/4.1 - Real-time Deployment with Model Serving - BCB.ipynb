{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d09c67-844e-4bb5-a00c-b772d59658ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# BEFORE YOU START: DBACADEMY ISSUE WORKROUND\n",
    "You must remember to comment out the command \"DA.reset_lesson()\" in the ../Includes/Classroom-Setup-01.py file before running this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce9288c-7148-45a4-90c1-c1847bff91eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Real-time Deployment with Model Serving\n",
    "\n",
    "In this demo, we will focus on real-time deployment of machine learning models. ML models can be deployed using various technologies. Databricks' Model Serving is an easy to use serverless infrastructure for serving the models in real-time.\n",
    "\n",
    "First, we will fit a model **without using a feature store**. Then, we will serve the model via Model Serving. Model serving **supports both the API and the UI**. To demonstrate both methods, we will, first, serve the model using the UI and then server the model using **Databricks' Python SDK**.\n",
    "\n",
    "In the second section, we will fit a model **with feature store and we will use online features during the inference.** For online features, **Databricks' Online Tables** can be used.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to;*\n",
    "\n",
    "* Implement a real-time deployment REST API using Model Serving.\n",
    "\n",
    "* Serve multiple model versions to a Serverless Model Serving endpoint.\n",
    "\n",
    "* Set up an A/B testing endpoint by splitting the traffic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c87e183-eea7-4f8a-b954-b5e5519e68eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **13.3.x-cpu-ml-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "809b3b14-0023-4ea0-a59b-5155e1a4a50f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc358f1b-8a0b-4fef-83b8-d46287982346",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: databricks-sdk in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7cc83a6c-16ce-4c43-a306-7f59923e57a0/lib/python3.10/site-packages (0.35.0)\nRequirement already satisfied: requests<3,>=2.28.1 in /databricks/python3/lib/python3.10/site-packages (from databricks-sdk) (2.28.1)\nRequirement already satisfied: google-auth~=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-7cc83a6c-16ce-4c43-a306-7f59923e57a0/lib/python3.10/site-packages (from databricks-sdk) (2.35.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk) (4.2.4)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk) (4.9)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk) (0.2.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.28.1->databricks-sdk) (1.26.11)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.28.1->databricks-sdk) (2022.9.14)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.28.1->databricks-sdk) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.28.1->databricks-sdk) (3.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk) (0.4.8)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install databricks-sdk --upgrade\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de7c645-a745-47c0-bc4a-632686238160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/machine-learning-model-deployment/v01\"\n\nValidating the locally installed datasets:\n| listing local files...(0 seconds)\n| validation completed...(0 seconds total)\nCreating & using the catalog \"realadmin_fcgj_da\"...(1 seconds)\n\nPredefined tables in \"realadmin_fcgj_da.default\":\n| db_academy_payload\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/realadmin@brianchristopherbrownhotmai.onmicrosoft.com/machine-learning-model-deployment\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/machine-learning-model-deployment/v01\n\nSetup completed (2 seconds)\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f22972af-4e08-489c-a6c7-41e3e810579c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12002205-448b-42d1-b6fb-10aec6e8cacb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          realadmin@brianchristopherbrownhotmai.onmicrosoft.com\nCatalog Name:      realadmin_fcgj_da\nSchema Name:       default\nWorking Directory: dbfs:/mnt/dbacademy-users/realadmin@brianchristopherbrownhotmai.onmicrosoft.com/machine-learning-model-deployment\nUser DB Location:  dbfs:/mnt/dbacademy-datasets/machine-learning-model-deployment/v01\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"User DB Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fadbb0a-f6de-4ab2-ada7-3b24e3b6d797",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "For this demonstration, we will use a fictional dataset from a Telecom Company, which includes customer information. This dataset encompasses **customer demographics**, including internet subscription details such as subscription plans, monthly charges and payment methods.\n",
    "\n",
    "After load the dataset, we will perform simple **data cleaning and feature selection**. \n",
    "\n",
    "In the final step, we will split the dataset to **features** and **response** sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d757ab-4351-473a-8549-779483752c86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Dataset path\n",
    "dataset_p_telco = f\"{DA.paths.datasets}/telco/telco-customer-churn.csv\"\n",
    "\n",
    "# Dataset specs\n",
    "primary_key = \"customerID\"\n",
    "response = \"Churn\"\n",
    "features = [\"SeniorCitizen\", \"tenure\", \"MonthlyCharges\", \"TotalCharges\"] # Keeping numerical only for simplicity and demo purposes\n",
    "\n",
    "\n",
    "# Read dataset (and drop nan)\n",
    "# Convert all fields to double for spark compatibility\n",
    "telco_df = spark.read.csv(dataset_p_telco, inferSchema=True, header=True, multiLine=True, escape='\"')\\\n",
    "            .withColumn(\"TotalCharges\", col(\"TotalCharges\").cast('double'))\\\n",
    "            .withColumn(\"SeniorCitizen\", col(\"SeniorCitizen\").cast('double'))\\\n",
    "            .withColumn(\"Tenure\", col(\"tenure\").cast('double'))\\\n",
    "            .na.drop(how='any')\n",
    "\n",
    "# Separate features and ground-truth\n",
    "features_df = telco_df.select(primary_key, *features)\n",
    "response_df = telco_df.select(primary_key, response)\n",
    "\n",
    "# Covert data to pandas dataframes\n",
    "X_train_pdf = features_df.drop(primary_key).toPandas()\n",
    "Y_train_pdf = response_df.drop(primary_key).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc839f2-4b74-4b15-bb23-41e577fa213b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Fit and Register Models\n",
    "\n",
    "Before we start model deployment process, we will **fit and register two models**. These models are called **\"Champion\"** and **\"Challenger\"** and they will be served later on using Databricks Model Serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced9c0b6-9036-4191-ba5b-883dc5ea52b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Setup Model Registry with UC\n",
    "\n",
    "Before we start model deployment, we need to fit and register a model. In this demo, **we will log models to Unity Catalog**, which means first we need to setup the **MLflow Model Registry URI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33dac144-cf70-4bb3-a56f-6c72e72869bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Point to UC model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    #"mlflow.set_experiment(\"/Users//4.1 - Real-time Deployment with Model Serving - BCB\")\n",
    #"mlflow.set_experiment(\"/Users/realadmin@brianchristopherbrownhotmai.onmicrosoft.com/machine-learning-model-deployment-1.0.3/04 - Real-time Deployment/4.1 - Real-time Deployment with Model Serving - BCB\")\n",
    "mlflow.set_experiment(\"/Users/realadmin@brianchristopherbrownhotmai.onmicrosoft.com/machine-learning-model-deployment-1.0.3/04 - Real-time Deployment/4.1 - Real-time Deployment with Model Serving - BCB\")\n",

    "client = mlflow.MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af71cea8-5c01-4133-804f-f1af583fcc97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fit and Register a Model with UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff99b27-0849-417d-8007-199a1520e88e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from mlflow.types.utils import _infer_schema\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.ml_model\" # Use 3-level namespace\n",
    "\n",
    "def get_latest_model_version(model_name):\n",
    "    \"\"\"Helper function to get latest model version\"\"\"\n",
    "    model_version_infos = client.search_model_versions(\"name = '%s'\" % model_name)\n",
    "    return str(max([int(model_version_info.version) for model_version_info in model_version_infos]))\n",
    "\n",
    "def fit_and_register_model(X, Y, model_name_=model_name, random_state_=42, model_alias=None, log_with_fs=False, training_set_spec_=None):\n",
    "    \"\"\"Helper function to train and register a decision tree model\"\"\"\n",
    "\n",
    "    clf = DecisionTreeClassifier(random_state=random_state_)\n",
    "    with mlflow.start_run(run_name=\"Demo4_1-Real-Time-Deployment\") as mlflow_run:\n",
    "\n",
    "        # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "        mlflow.sklearn.autolog(\n",
    "            log_input_examples=True,\n",
    "            log_models=False,\n",
    "            log_post_training_metrics=True,\n",
    "            silent=True)\n",
    "        \n",
    "        clf.fit(X, Y)\n",
    "\n",
    "        # Log model and push to registry\n",
    "        if log_with_fs:\n",
    "            # Infer output schema\n",
    "            try:\n",
    "                output_schema = _infer_schema(Y)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Could not infer model output schema: {e}\")\n",
    "                output_schema = None\n",
    "            \n",
    "            # Log using feature engineering client and push to registry\n",
    "            fe = FeatureEngineeringClient()\n",
    "            fe.log_model(\n",
    "                model=clf,\n",
    "                artifact_path=\"decision_tree\",\n",
    "                flavor=mlflow.sklearn,\n",
    "                training_set=training_set_spec_,\n",
    "                output_schema=output_schema,\n",
    "                registered_model_name=model_name_\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            signature = infer_signature(X, Y)\n",
    "            mlflow.sklearn.log_model(\n",
    "                clf,\n",
    "                artifact_path=\"decision_tree\",\n",
    "                signature=signature,\n",
    "                registered_model_name=model_name_\n",
    "            )\n",
    "\n",
    "        # Set model alias\n",
    "        if model_alias:\n",
    "            time.sleep(20) # Wait 20 secs for model version to be created\n",
    "            client.set_registered_model_alias(model_name_, model_alias, get_latest_model_version(model_name_))\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff6f5f51-651d-426a-99d1-ce65ca174347",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#client.delete_registered_model(model_name) # save for break-glass purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2efc71f9-21e0-49b3-9415-4608cfb284d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A model version with name='realadmin_fcgj_da.default.ml_model' and alias='champion' already exists\n"
     ]
    }
   ],
   "source": [
    "#model_champion = fit_and_register_model(X_train_pdf, Y_train_pdf, model_name, 42, \"Champion\")\n",
    "\n",
    "# We will only create the Champion version if a model version with the same name and the label \"Champion\" does not already exist\n",
    "\n",
    "#model_champion_uri = F\"models:/{model_name}@B06U5\" # output is a handle even if there is no such label\n",
    "#model_champion_uri\n",
    "\n",
    "#model_versions = [v for v in client.search_model_versions(\"name = 'realadmin_fcgj_da.default.ml_model'\")] # output 'aliases' is an empty list\n",
    "#model_versions\n",
    "\n",
    "#client.get_model_version_by_alias(name=model_name, alias=\"B06U5\") # returns an error if the alias does not exist\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "w = WorkspaceClient()\n",
    "#[v for v in w.model_versions.list(full_name=model_name)] # again, output 'aliases' is an empty list\n",
    "\n",
    "champion_exists = False\n",
    "\n",
    "models = [m for m in w.registered_models.list() if m.as_dict()[\"full_name\"]==model_name] # here, registered_models returns models, not model versions\n",
    "if models:\n",
    "  versions = w.registered_models.get(full_name=model_name, include_aliases=True)         # here, registered_models returns model versions, not models\n",
    "  versions_as_dicts = versions.as_dict()\n",
    "  if \"aliases\" in versions_as_dicts:\n",
    "    champion_alias = \"champion\" # keep this in lower case, in spite of examples you see\n",
    "    champion_model_versions = [v for v in versions_as_dicts[\"aliases\"] if v[\"alias_name\"] == champion_alias]\n",
    "    if champion_model_versions:\n",
    "      print(F\"A model version with name='{model_name}' and alias='{champion_alias}' already exists\")\n",
    "      champion_exists = True\n",
    "    else:\n",
    "      print(F\"There don't appear to be any versions of model '{model_name}' with the '{champion_alias}' alias at this time.\")\n",
    "  else:\n",
    "    print(F\"There don't appear to be any versions of model '{model_name}' with aliases at this time.\")\n",
    "else:\n",
    "  print(F\"How very odd.  There don't appear to be any models at this time.\")\n",
    "\n",
    "if not champion_exists:\n",
    "  print(F\"Creating model version with name='{model_name}' and alias='{champion_alias}'...\")\n",
    "  model_champion = fit_and_register_model(X_train_pdf, Y_train_pdf, model_name, 42, model_alias=champion_alias)\n",
    "  print(F\"... created a model version with name='{model_name}' and alias='{champion_alias}'\")\n",
    "  ## fun fact: aliases might a lot of time to fully register in Unity Catalog\n",
    "  #import time\n",
    "  #print(\"sleeping for 60 seconds to let the new version fully register in Unity Catalog (UC)...\")\n",
    "  #time.sleep(60)\n",
    "  #print(\"...waking up now.  Sorry to keep you waiting.\")\n",
    "  #champion_model_versions_again = [v[\"version_num\"] for v in model_versions_not_models.as_dict()[\"aliases\"] if v[\"alias_name\"]==champion_alias]\n",
    "  #if champion_model_versions:\n",
    "  #  print(F\"A model version with name='{model_name}' and alias='{champion_alias}' exists in UC's model registry now\")\n",
    "  #else:\n",
    "  #  print(F\"A model version with name='{model_name}' and alias='{champion_alias}' has STILL not fully registered in UC's model registry, but it should only take another moment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc5da4a-2828-444f-aba1-006fffe1c8c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\nRegistered model 'realadmin_fcgj_da.default.ml_model' already exists. Creating a new version of this model...\n2024/10/18 20:01:49 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: realadmin_fcgj_da.default.ml_model, version 15\nCreated version '15' of model 'realadmin_fcgj_da.default.ml_model'.\n"
     ]
    }
   ],
   "source": [
    "# We will always create a new challenger model\n",
    "\n",
    "model_challenger = fit_and_register_model(X_train_pdf, Y_train_pdf, model_name, 10, model_alias=\"challenger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644283a3-d86d-48f2-94dc-2082a3190a67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## LESSON LEARNED: DO NOT ATTEMPT TO DELETE MODEL SERVING ENDPOINTS IN AZURE DATABRICKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c21635d2-86c5-490f-a896-de4074a6c173",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### DO NOT REMOVE THE False and False and False FROM THIS CELL\n",
    "\n",
    "if False and False and False and False and False:\n",
    "\n",
    "  #if endpoint_name in [e.as_dict()[\"name\"] for e in w.serving_endpoints.list() if e.as_dict()[\"name\"] == endpoint_name]:\n",
    "  if [e for e in w.serving_endpoints.list() if e.as_dict()[\"name\"] == endpoint_name]:\n",
    "    print(f\"Deleting endpoint {endpoint_name}...\")\n",
    "    w.serving_endpoints.delete(endpoint_name) ###### DO NOT ATTEMPT TO DELETE ANY MODEL SERVING ENDPOINT IN AZURE DATABRICKS, EITHER BY API OR UI - WILL EXPLAIN LATER\n",
    "    print(f\"... deleted endpoint {endpoint_name}\")\n",
    "  else:\n",
    "    print(f\"Endpoint {endpoint_name} did not already exist\")\n",
    "\n",
    "# if you do delete the endpoint, against my advice, compare the output line of this cell to the output line of the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a6ffeb-87ff-4b5f-ad46-33b43515761c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An endpoint with the name 'ML_AS_03_Demo4_realadmin_fcgj_da_mlmdp' already exists, so we will attempt to update it.\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Create/Update endpoint and deploy model+version\n",
    "w = WorkspaceClient()\n",
    "endpoint_name = f\"ML_AS_03_Demo4_{DA.unique_name('_')}\"\n",
    "#endpoint_name = f\"BCB_{DA.unique_name('_')}\"\n",
    "\n",
    "# Parse model name from UC namespace\n",
    "served_model_name =  model_name.split('.')[-1]\n",
    "\n",
    "endpoint_exists = True if [e for e in w.serving_endpoints.list() if e.as_dict()[\"name\"] == endpoint_name] else False\n",
    "\n",
    "if endpoint_exists:\n",
    "  print(F\"An endpoint with the name '{endpoint_name}' already exists, so we will attempt to update it.\")\n",
    "else:\n",
    "  print(F\"An endpoint with the name '{endpoint_name}' was not found, so we will attempt to create it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a904014f-f966-43c0-8971-b68b34cbee77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Real-time A/B Testing Deployment with Model Serving\n",
    "\n",
    "Let's serve the two models we logged in the previous step using Model Serving. Model Serving supports endpoint management via the UI and the API. \n",
    "\n",
    "Below you will find instructions for using the UI and it is simpler method compared to the API. **In this demo, we will use the API to configure and create the endpoint**.\n",
    "\n",
    "**Both the UI and the API support querying created endpoints in real-time**. We will use the API to query the endpoint using a test-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2c16fed-f39d-4634-b69e-e5db2ae42e47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Option 1: Serve model(s) using UI\n",
    "\n",
    "After registering the (new version(s) of the) model to the model registry. To provision a serving endpoint via UI, follow the steps below.\n",
    "\n",
    "1. In the left sidebar, click **Serving**.\n",
    "\n",
    "2. To create a new serving endpoint, click **Create serving endpoint**.   \n",
    "  \n",
    "    a. In the **Name** field, type a name for the endpoint.  \n",
    "  \n",
    "    b. Click in the **Entity** field. A dialog appears. Select **Unity catalog model**, and then select the catalog, schema, and model from the drop-down menus.  \n",
    "  \n",
    "    c. In the **Version** drop-down menu, select the version of the model to use.  \n",
    "  \n",
    "    d. Click **Confirm**.  \n",
    "  \n",
    "    e. In the **Compute Scale-out** drop-down, select Small, Medium, or Large. If you want to use GPU serving, select a GPU type from the **Compute type** drop-down menu.\n",
    "  \n",
    "    f. *[OPTIONAL]* to deploy another model (e.g. for A/B testing) click on **+Add Served Entity** and fill the above mentioned details.\n",
    "  \n",
    "    g. Click **Create**. The endpoint page opens and the endpoint creation process starts.   \n",
    "  \n",
    "See the Databricks documentation for details ([AWS](https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html#ui-workflow)|[Azure](https://learn.microsoft.com/azure/databricks/machine-learning/model-serving/create-manage-serving-endpoints#--ui-workflow))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38ca6d5-7249-48d0-9566-7d9202ba4850",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Option 2: Serve Model(s) Using the *Databricks Python SDK*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f7a865-a873-4010-ac2a-6725488426ed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Get Models to Serve\n",
    "\n",
    "We will serve two models, therefore, we will get model version of the two models (**Champion** and **Challenger**) that we registered in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8adcdef9-f74d-406b-9e18-151e844f0742",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_version_champion = client.get_model_version_by_alias(name=model_name, alias=\"champion\").version # Get champion version\n",
    "model_version_challenger = client.get_model_version_by_alias(name=model_name, alias=\"challenger\").version # Get challenger version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ca239c-e4fe-4757-a7b0-167154555079",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# save for tagging\n",
    "if False:\n",
    "    from datetime import datetime\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(dt_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667ed866-5501-4f69-91dd-641e2ea61420",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Configure and Create Serving Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f033fb-36c3-47f2-934e-5c2b88f542a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not endpoint_exists:\n",
    "    from databricks.sdk.service.serving import EndpointCoreConfigInput\n",
    "\n",
    "    endpoint_config_dict = {\n",
    "        \"served_models\": [\n",
    "            {\n",
    "                \"model_name\": model_name,\n",
    "                \"model_version\": model_version_champion,\n",
    "                \"scale_to_zero_enabled\": True,\n",
    "                \"workload_size\": \"Small\"\n",
    "            },\n",
    "            {\n",
    "                \"model_name\": model_name,\n",
    "                \"model_version\": model_version_challenger,\n",
    "                \"scale_to_zero_enabled\": True,\n",
    "                \"workload_size\": \"Small\"\n",
    "            },\n",
    "        ],\n",
    "        \"traffic_config\": {\n",
    "            \"routes\": [\n",
    "                {\"served_model_name\": f\"{served_model_name}-{model_version_champion}\", \"traffic_percentage\": 90},\n",
    "                {\"served_model_name\": f\"{served_model_name}-{model_version_challenger}\", \"traffic_percentage\": 10},\n",
    "            ]\n",
    "        },\n",
    "        \"auto_capture_config\":{\n",
    "            \"catalog_name\": DA.catalog_name,\n",
    "            \"schema_name\": DA.schema_name,\n",
    "            \"table_name_prefix\": \"db_academy\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # I think one reason why Databricks abandoned the \"models\" format is because there is no key shared between models and routes, just positional associations\n",
    "\n",
    "    endpoint_config = EndpointCoreConfigInput.from_dict(endpoint_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c2156b9-aed7-4fd8-894b-509589401b70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not endpoint_exists:\n",
    "\n",
    "    from databricks.sdk.service.serving import EndpointTag\n",
    "\n",
    "    try:\n",
    "      \n",
    "      print(f\"Creating endpoint {endpoint_name} with models {model_name} versions {model_version_champion} ('champion') & {model_version_challenger} ('challenger')...\")\n",
    "\n",
    "      w.serving_endpoints.create_and_wait(\n",
    "        name=endpoint_name,\n",
    "        config=endpoint_config,\n",
    "        tags=[EndpointTag.from_dict({\"key\": \"db_academy\", \"value\": \"serve_model_example\", \"updated\": dt_string})]\n",
    "      )\n",
    "      \n",
    "      print(f\"... created endpoint {endpoint_name} with models {model_name} versions {model_version_champion} ('champion') & {model_version_challenger} ('challenger')\")\n",
    "\n",
    "    except Exception as e:\n",
    "      \n",
    "      # if the endpoint fails to fully deploy, it will \"exist\" (as a definition to be edited) and \"not exist\" (as a working service) at the same time\n",
    "      # if the endpoint is deleted, via the UI or the API, it is partially deleted and there doesn't seem to be any way to fix it\n",
    "      # waiting 2 hours after deleting an endpoint was insufficient to get over this issue\n",
    "      # once a model enters the exists / not exists stage, there doesn't seem to be a way to deploy any other models via API\n",
    "      # they all seem to return an already-exists error, even if the name is made up at the last second\n",
    "      # I tried deleting the metastore but not the workspace, and I tried deleting the workspace but not the metastore; the problem lingered\n",
    "      # ONLY BY DELETING THE WORKSPACE AND THE METASTORE WAS I FINALLY ABLE TO RESOLVE THIS PROBLEM\n",
    "\n",
    "      if \"already exists\" in e.args[0]:\n",
    "        print(f\"An endpoint with name='{endpoint_name}' already exists, at least in some sense of that word.  Did someone attempt to delete the endpoint?\")\n",
    "\n",
    "      else:\n",
    "        raise(e)\n",
    "    \n",
    "# if you do delete the endpoint, against my advice, compare the output line of this cell to the output line of the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1bfe2b1-378b-41e2-b09e-69dd12eb7774",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ServedEntityInput(entity_name='realadmin_fcgj_da.default.ml_model', entity_version='3', environment_vars=None, external_model=None, instance_profile_arn=None, max_provisioned_throughput=None, min_provisioned_throughput=None, name='ml_model-3-champion', scale_to_zero_enabled=True, workload_size='Small', workload_type='CPU'), ServedEntityInput(entity_name='realadmin_fcgj_da.default.ml_model', entity_version='15', environment_vars=None, external_model=None, instance_profile_arn=None, max_provisioned_throughput=None, min_provisioned_throughput=None, name='ml_model-15-challenger', scale_to_zero_enabled=True, workload_size='Small', workload_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "# If you are looking for documentation fthat covers ServedEntityInput, I didn't find much (October 17, 2024)\n",
    "\n",
    "name_champion = f\"{served_model_name}-{model_version_champion}-champion\"\n",
    "name_challenger = f\"{served_model_name}-{model_version_challenger}-challenger\"\n",
    "\n",
    "assert len(name_champion) <= 63, \"champion name exceeds 63 characters\"\n",
    "assert len(name_challenger) <= 63, \"challenger name exceeds 63 characters\"\n",
    " \n",
    "if endpoint_exists:\n",
    "\n",
    "    from databricks.sdk.service.serving import ServedEntityInput\n",
    "    served_entities_config = [\n",
    "        ServedEntityInput(\n",
    "            entity_name = F\"{model_name}\",\n",
    "            entity_version = F\"{model_version_champion}\",\n",
    "            name = name_champion,\n",
    "            scale_to_zero_enabled = True,\n",
    "            workload_size = 'Small',\n",
    "            workload_type = 'CPU'\n",
    "        ),\n",
    "        ServedEntityInput(\n",
    "            entity_name = F\"{model_name}\",\n",
    "            entity_version = F\"{model_version_challenger}\",\n",
    "            name = name_challenger,\n",
    "            scale_to_zero_enabled = True,\n",
    "            workload_size = 'Small',\n",
    "            workload_type = 'CPU'\n",
    "        )\n",
    "    ]\n",
    "    print(served_entities_config)\n",
    "\n",
    "# bit of advice: make sure name is unique, or otherwise routes (next cell) will be confused with prior versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33e08a6b-3af0-4672-b044-5cbc3421e58d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrafficConfig(routes=[Route(served_model_name='ml_model-3-champion', traffic_percentage=90), Route(served_model_name='ml_model-15-challenger', traffic_percentage=10)])\n"
     ]
    }
   ],
   "source": [
    "# If you are looking for documentation fthat covers TrafficConfig and Route, I didn't find any (October 17, 2024)\n",
    "\n",
    "if endpoint_exists:\n",
    "  from databricks.sdk.service.serving import TrafficConfig\n",
    "  from databricks.sdk.service.serving import Route\n",
    "\n",
    "  traffic_config_dict = [\n",
    "    Route(served_model_name=name_champion, traffic_percentage=90),\n",
    "    Route(served_model_name=name_challenger, traffic_percentage=10)\n",
    "  ]\n",
    "  traffic_config = TrafficConfig(traffic_config_dict)\n",
    "  print(traffic_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edc4ef07-0572-440b-81a4-af8daa7c6115",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating endpoint ML_AS_03_Demo4_realadmin_fcgj_da_mlmdp with models realadmin_fcgj_da.default.ml_model versions 3 ('champion') & 15 ('challenger')...\n... updated endpoint ML_AS_03_Demo4_realadmin_fcgj_da_mlmdp with models realadmin_fcgj_da.default.ml_model versions 3 ('champion') & 15 ('challenger')\n"
     ]
    }
   ],
   "source": [
    "if endpoint_exists:\n",
    "    from databricks.sdk.service.serving import EndpointTag\n",
    "\n",
    "    print(f\"Updating endpoint {endpoint_name} with models {model_name} versions {model_version_champion} ('champion') & {model_version_challenger} ('challenger')...\")\n",
    "\n",
    "    w.serving_endpoints.update_config_and_wait(\n",
    "        name=endpoint_name,\n",
    "        served_entities=served_entities_config,\n",
    "        traffic_config=traffic_config\n",
    "    )\n",
    "\n",
    "    # TO DO: add tag with new update date-time\n",
    "\n",
    "    print(f\"... updated endpoint {endpoint_name} with models {model_name} versions {model_version_champion} ('champion') & {model_version_challenger} ('challenger')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4407d977-7735-4882-a2b7-ee06a0c5e56b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Verify Endpoint Creation / Update\n",
    "\n",
    "Let's verify that the endpoint is created/updated and ready to be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c829eee6-e0ad-49b7-8ae5-befdca5c8d9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "endpoint = w.serving_endpoints.wait_get_serving_endpoint_not_updating(endpoint_name)\n",
    "\n",
    "assert endpoint.state.config_update.value == \"NOT_UPDATING\" and endpoint.state.ready.value == \"READY\" , \"Endpoint not ready or failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "047e5ddd-9ed1-4817-9698-90c105e19b16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Query the Endpoint\n",
    "\n",
    "Here we will use a very simple `test-sample` to use for inference. In a real-life scenario, you would typically load this set from a table or a streaming pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9acdfff5-6f70-4b4f-97a6-9b918c47d0ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference results:\n['No', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "# Hard-code test-sample\n",
    "dataframe_records = [\n",
    "    {\"SeniorCitizen\": 0, \"tenure\":12, \"MonthlyCharges\":65, \"TotalCharges\":800},\n",
    "    {\"SeniorCitizen\": 1, \"tenure\":24, \"MonthlyCharges\":40, \"TotalCharges\":500}\n",
    "]\n",
    "\n",
    "print(\"Inference results:\")\n",
    "query_response = w.serving_endpoints.query(name=endpoint_name, dataframe_records=dataframe_records)\n",
    "print(query_response.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3657244-3e43-4e1f-aa7b-23fc58c71996",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean up Classroom\n",
    "\n",
    "Run the following cell to remove lessons-specific assets created during this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c7b981d-200a-4e98-8522-79a7e43a832d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae6ab95f-8740-40e8-a579-dc8495bd5afa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this demo, we covered how to serve ML models in real-time using Databricks Model Serving. In the first part, we demonstrated how to serve models without feature store integration. Furthermore, we showed how to deploy two models on the same endpoint to conduct an A/B testing scenario. In the second section of the demo, we deployed a model with feature store integration using Databricks Online Tables. Additionally, we demonstrated how to use the endpoint for inference with Online Tables integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382e03ba-1680-4b7e-b89f-c0273d8dfc6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2024 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e62eba5-1452-4b0d-a048-0d7bdc8099e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4.1 - Real-time Deployment with Model Serving - BCB",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
